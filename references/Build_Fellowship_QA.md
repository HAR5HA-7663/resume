# Build Fellowship Applications - Q&A for Interview Prep

---

# Application 1: Build and Train Transformer Language Model

**Program:** Build and train transformer language model
**Fellowship:** The Build Fellowship (Open Avenues Foundation)
**Duration:** 8 weeks (February 2 - March 23, 2026)
**Schedule:** Tuesdays, 6:00 PM ET / 3:00 PM PT
**Commitment:** 2-3 hours per week
**Level:** Expert - Advanced experience required
**Status:** Applied (January 9, 2026)
**Results:** Week of January 16, 2026

---

## Question 1: What are 2-3 courses you have taken (or experiences you have) that will help you succeed in this project?

**My Answer:**
I've taken graduate-level Machine Learning and NLP courses at Lawrence Tech where I implemented attention mechanisms and sequence models. As a Graduate Research Assistant, I've been fine-tuning LLMs - recently did QLoRA fine-tuning on Qwen3-4B for a resume optimization project that achieved 9.5/10 quality scores. I also built RAG systems using embeddings and transformer-based rerankers, so I have solid hands-on experience with the HuggingFace Transformers ecosystem.

**Key Points to Remember:**
- Graduate ML and NLP coursework at Lawrence Tech
- Implemented attention mechanisms and sequence models in class
- Graduate Research Assistant working on LLMs
- QLoRA fine-tuning on Qwen3-4B (4-bit NF4 quantization)
- Resume optimization project with 9.5/10 quality scores
- Built RAG systems with embeddings and rerankers
- HuggingFace Transformers ecosystem experience

---

## Question 2: In 2-3 sentences, describe how this project aligns with your academic and career objectives.

**My Answer:**
I'm finishing my MS in Computer Science focusing on AI/ML, and my research involves building agentic AI systems powered by LLMs. Building a transformer from scratch would give me deeper intuition about the models I work with daily - I want to move from being someone who uses transformers to someone who truly understands the attention mechanisms and training dynamics under the hood. My career goal is to become an ML Engineer specializing in NLP and generative AI.

**Key Points to Remember:**
- MS in Computer Science at Lawrence Tech (graduating December 2025)
- Research focus: Agentic AI systems powered by LLMs
- Want deeper understanding of transformer internals
- Moving from user to true expert
- Career goal: ML Engineer in NLP and Generative AI

---

## Question 3: What are 2-3 things you expect to learn from this project?

**My Answer:**
First, I want to understand the complete training pipeline from tokenization through attention computation to text generation - I've fine-tuned models but never built one from the ground up. Second, I'm really interested in learning efficient training techniques for limited compute since most of my work has been with pre-trained models on cloud GPUs. Third, I want to get better at evaluating language model quality beyond just looking at perplexity - understanding what makes a model actually generate coherent, useful text.

**Key Points to Remember:**
1. **Complete training pipeline:** Tokenization → Attention → Generation (never built from scratch)
2. **Efficient training on limited compute:** Most experience is with pre-trained models on cloud GPUs
3. **Language model evaluation:** Beyond perplexity, understanding coherent text generation

---

## What You Will Learn (from program description)

- Acquire familiarity with PyTorch machine learning library, visualization tools like matplotlib and plotly
- Learn how to collect, preprocess, and tokenize text data for training a language model
- Implement and compare baseline models or traditional NLP methods to highlight improvements offered by transformer-based approaches

---

# Application 2: Integrating LLM Service Using Python

**Program:** Integrating Large Language Model (LLM) service using Python
**Fellowship:** The Build Fellowship (Open Avenues Foundation)
**Duration:** 8 weeks (February 2 - March 23, 2026)
**Schedule:** Mondays, 6:00 PM ET / 3:00 PM PT
**Commitment:** 2-3 hours per week
**Level:** Intermediate - Some experience recommended
**Status:** Applied (January 9, 2026)
**Results:** Week of January 16, 2026

---

## Question 1: What are 2-3 courses you have taken (or experiences you have) that will help you succeed in this project?

**My Answer:**
I've been integrating LLMs into production applications for over a year now. As a Graduate Research Assistant, I built multi-agent systems using LangChain and CrewAI that orchestrate multiple LLM calls. I've worked with GPT-4, Gemini, and Claude APIs - for example, I built Resumade.in which uses a multi-LLM pipeline to optimize resumes. Python is my primary language with 5+ years of experience, and I've built FastAPI backends that serve LLM-powered features.

**Key Points to Remember:**
- 1+ year integrating LLMs into production apps
- Multi-agent systems with LangChain and CrewAI
- Experience with GPT-4, Gemini, Claude APIs
- Built Resumade.in (multi-LLM pipeline)
- Python: 5+ years experience
- FastAPI backends serving LLM features

---

## Question 2: In 2-3 sentences, describe how this project aligns with your academic and career objectives.

**My Answer:**
My research at Lawrence Tech is literally about building agentic AI systems that integrate LLMs - so this project is directly in my wheelhouse. I want to see how others approach the same problems I'm solving in my research, especially around structuring LLM calls for real-world tasks like email automation. My career goal is to become an ML Engineer building production LLM applications, and hands-on project experience with mentorship would be invaluable.

**Key Points to Remember:**
- Research directly aligned: Agentic AI systems integrating LLMs
- Want to learn different approaches to same problems
- Interested in real-world task structuring (email automation)
- Career goal: ML Engineer building production LLM apps
- Values mentorship and hands-on experience

---

## Question 3: What are 2-3 things you expect to learn from this project?

**My Answer:**
First, I want to learn best practices for production LLM integrations - things like prompt engineering patterns, handling rate limits gracefully, and managing API costs. Second, I'm curious about designing good user experiences around LLM latency since my current projects sometimes feel slow. Third, I want to learn how to build robust error handling for when LLMs return unexpected outputs - this is something I've struggled with in my own projects.

**Key Points to Remember:**
1. **Production best practices:** Prompt engineering patterns, rate limits, API cost management
2. **UX around latency:** Making LLM-powered apps feel responsive
3. **Robust error handling:** Dealing with unexpected LLM outputs

---

## What You Will Learn (from program description)

- Write Python code that integrates with third-party LLM services (OpenAI, Anthropic)
- Understand best practice of software development and API integration
- Explore and practice prompt engineering for Large Language Model

---

# Interview Prep Tips (Both Applications)

1. **Be specific about your experience:** Mention specific models (Qwen3-4B, GPT-4), frameworks (LangChain, CrewAI), and metrics
2. **Connect to real projects:** Resume Optimizer, Resumade.in, CRE Research Agent, multi-agent systems
3. **Show self-awareness:** Acknowledge what you want to learn vs. what you already know
4. **Career clarity:** ML Engineer specializing in NLP/Generative AI/Production LLM apps
5. **Technical depth:** Can discuss attention mechanisms, embeddings, API integrations, error handling

---

## Relevant Projects to Mention in Interview

1. **Resume Optimizer (QLoRA Fine-tuning)**
   - Fine-tuned Qwen3-4B using QLoRA with 4-bit NF4 quantization
   - Achieved 9.5/10 quality scores
   - GPU memory optimization (18-22GB)

2. **Resumade.in (Multi-LLM Pipeline)**
   - Production app using GPT-4 and Gemini
   - FastAPI backend
   - Multi-step LLM orchestration

3. **CRE Research Agent (RAG)**
   - Vector databases (Qdrant, pgvector)
   - Embeddings and reranking
   - Academic literature review automation

4. **Graduate Research (Agentic AI)**
   - Multi-agent systems with CrewAI + LangChain MCP
   - AWS Fargate/EKS deployment
   - OpenSearch Serverless for semantic search

---

*Applications submitted: January 9, 2026*
*Last updated: January 9, 2026*
